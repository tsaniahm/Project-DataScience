knitr::opts_chunk$set(echo = TRUE)
#tm -> untuk membersihkan data
library(tm)
#vroom -> untuk meload dataset
library(vroom)
#here -> untuk menyimpan dataset
library(here)
dataset <- vroom(here('Amazon_Alexa_Review.csv'))
data_reviews <- dataset$verified_reviews
data_reviews2 <- Corpus(VectorSource(data_reviews))
# === Data Cleaning ===
# Membersihkan URL
deleteURL <- function(x) gsub("http[^[:space:]]*", "", x)
cleanReview <- tm_map(data_reviews2, deleteURL)
#membersihkan New Line
deleteNewLine <- function(y) gsub("\n", "", y)
cleanReview <- tm_map(cleanReview, deleteNewLine)
# Membersihkan tanda koma
replaceComma <- function(y) gsub(",", "", y)
cleanReview <- tm_map(cleanReview, replaceComma)
# Membersihkan tanda titik dua
deleteColon <- function(y) gsub(":", "", y)
cleanReview <- tm_map(cleanReview, deleteColon)
# Membersihkan tanda titik koma
deleteSemiColon <- function(y) gsub(";", " ", y)
cleanReview <- tm_map(cleanReview, deleteSemiColon)
# Membersihkan tanda titik tiga
deleteEllipsis <- function(y) gsub("p…", "", y)
cleanReview <- tm_map(cleanReview, deleteEllipsis)
# Membersihkan amp
deleteAmp <- function(y) gsub("&amp;", "", y)
cleanReview <- tm_map(cleanReview, deleteAmp)
# Membersihkan karakter
deleteUN <- function(z) gsub("@\\w+", "", z)
cleanReview <- tm_map(cleanReview, deleteUN)
remove.all <- function(xy) gsub("[^[:alpha:][:space:]]*", "", xy)
# Membersihkan Tanda Baca
cleanReview <- tm_map(cleanReview,remove.all)
cleanReview <- tm_map(cleanReview, removePunctuation)
cleanReview <- tm_map(cleanReview, tolower)
Stopwords = readLines("stopwords.txt")
cleanReview <- tm_map(cleanReview,removeWords,Stopwords)
dataframe<-data.frame(text=unlist(sapply(cleanReview, `[`)), stringsAsFactors=F)
View(dataframe)
# Menyimpan data yang telah dibersihkan
write.csv(dataframe,file = 'clean_reviews.csv')
# Library naive bayes
library(e1071)
# Library klasifikasi data
library(caret)
# Library untuk fungsi get_nrc
library(syuzhet)
dataUlasan <- read.csv("clean_reviews.csv", stringsAsFactors = FALSE)
# Mengubah text menjadi char
review <- as.character(dataUlasan$text)
emotion <- get_nrc_sentiment(review)
# Melakukan klasifikasi data
review_combine <- cbind(dataUlasan$text, emotion)
par(mar=rep(3,4))
a <- barplot(colSums(emotion), col=rainbow(10), ylab='count', main='Analisis Sentimen')
brplt <- a
# Library untuk corpus dalam cleaning data
library(tm)
library(RTextTools)
# Library algoritma naivebayes
library(e1071)
library(dplyr)
library(caret)
#membaca file csv
df <- read.csv("clean_reviews.csv",stringsAsFactors = FALSE)
#melihat tipe dan juga struktur objek
glimpse(df)
#Mengatur seed generator bilangan acak R(berfungsi agar bilangan acak R akan tetap sama walau diproduksi berulang kali)
set.seed(20)
df <- df[sample(nrow(df)),]
df <- df[sample(nrow(df)),]
glimpse(df)
corpus <- Corpus(VectorSource(df$text))
corpus
inspect(corpus[1:10])
# Membersihkan kembali data yang tidak dibutuhkan
corpus.clean <- corpus%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords,stopwords(kind="en"))%>%
tm_map(stripWhitespace)
dtm <- DocumentTermMatrix(corpus.clean)
inspect(dtm[1:10,1:20])
#Untuk klasifikasi data menggunakan data training dan data test
df.train <- df[1:50,]
df.test <- df[51:100,]
dtm.train <- dtm[1:50,]
dtm.test <- dtm[51:100,]
corpus.clean.train <- corpus.clean[1:50]
corpus.clean.test <- corpus.clean[51:100]
#Kemudian teks akan diubah menjadi DTM(Document-Term Matrix) dengan proses tokenization -> akan mengubah kalimat menjadi beberapa term(1 kata, 2 kata, dst).
dim(dtm.train)
fivefreq <- findFreqTerms(dtm.train,5)
length(fivefreq)
dtm.train.nb <- DocumentTermMatrix(corpus.clean.train,control = list(dictionary=fivefreq))
#dim(dtm.train.nb)
dtm.test.nb <- DocumentTermMatrix(corpus.clean.test,control = list(dictionary=fivefreq))
dim(dtm.test.nb)
convert_count <- function(x){
y <- ifelse(x>0,1,0)
y <- factor(y,levels=c(0,1),labels=c("no","yes"))
y
}
#Data train dan test naive bayes
trainNB <- apply(dtm.train.nb,2,convert_count)
testNB <- apply(dtm.test.nb,1,convert_count)
#menampilkan wordcloud
library(wordcloud)
wordcloud(corpus.clean, min.freq = 4, max.words=100, random.order=F, colors=brewer.pal(8,"Dark2"))
#library yang dibutuhkan
library(shiny)
library(here)
library(vroom)
library(dplyr)
library(ggplot2)
library(plotly)
library(syuzhet)
library(wordcloud)
#membaca data csv
Alexa_review<- vroom(here("clean_reviews.csv"))
Alexa_review <- data.frame(Alexa_review)
#bagian UI
ui <- fluidPage(
titlePanel("Analisis Sentimen Terhadap Amazon Alexa"),
mainPanel(
tabsetPanel(type = "tabs",
# Data Review
tabPanel("Data Amazon Alexa", DT::dataTableOutput('tbl2')),
# Data Clean
tabPanel("Data Clean", DT::dataTableOutput('tbl')),
# Barplot
tabPanel("Barplot", plotOutput("scatterplot")),
# Word Cloud
tabPanel("Wordcloud", plotOutput("Wordcloud"))
)
)
)
# SERVER
server <- function(input, output) {
# Output Data Tabel
output$tbl2 = DT::renderDataTable({
DT::datatable(dataset <-
vroom(here('Amazon_Alexa_Review.csv')), options =
list(lengthChange = FALSE))
})
# Output Data Clean
output$tbl = DT::renderDataTable({
DT::datatable(Alexa_review, options = list(lengthChange =
FALSE))
})
# Output Plot
output$scatterplot <- renderPlot({
par(mar=rep(3,4))
a <- barplot(colSums(s),col=rainbow(10),ylab='count',main='Sentimen
Analisis')}, height=400)
df <- read.csv("clean_reviews.csv",stringsAsFactors = FALSE)
glimpse(df)
# Set the seed of R‘s random number generator, which is useful for creating simulations or random objects that can be reproduced.
set.seed(20)
df <- df[sample(nrow(df)),]
df <- df[sample(nrow(df)),]
glimpse(df)
corpus <- Corpus(VectorSource(df$text))
corpus
inspect(corpus[1:10])
# Membersihkan data yang tidak dibutuhkan
corpus.clean <- corpus%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords,stopwords(kind="en"))%>%
tm_map(stripWhitespace)
dtm <- DocumentTermMatrix(corpus.clean)
inspect(dtm[1:10,1:20])
df.train <- df[1:50,]
df.test <- df[51:100,]
dtm.train <- dtm[1:50,]
dtm.test <- dtm[51:100,]
corpus.clean.train <- corpus.clean[1:50]
corpus.clean.test <- corpus.clean[51:100]
dim(dtm.train)
fivefreq <- findFreqTerms(dtm.train,5)
length(fivefreq)
dtm.train.nb <- DocumentTermMatrix(corpus.clean.train,control = list(dictionary=fivefreq))
#dim(dtm.train.nb)
dtm.test.nb <- DocumentTermMatrix(corpus.clean.test,control = list(dictionary=fivefreq))
dim(dtm.test.nb)
convert_count <- function(x){
y <- ifelse(x>0,1,0)
y <- factor(y,levels=c(0,1),labels=c("no","yes"))
y
}
trainNB <- apply(dtm.train.nb,2,convert_count)
testNB <- apply(dtm.test.nb,1,convert_count)
# Output WordCloud
output$Wordcloud <- renderPlot({
wordcloud(corpus.clean,min.freq =
4,max.words=100,random.order=F,colors=brewer.pal(8,"Dark2"))
})
}
# Program Shiny
shinyApp(ui = ui, server = server)
View(deleteUN)
#tm -> untuk membersihkan data
library(tm)
#vroom -> untuk meload dataset
library(vroom)
#here -> untuk menyimpan dataset
library(here)
#tm -> untuk membersihkan data
library(tm)
#vroom -> untuk meload dataset
library(vroom)
#here -> untuk menyimpan dataset
library(here)
dataset <- vroom(here('Amazon_Alexa_Review.csv'))
data_reviews <- dataset$verified_reviews
data_reviews2 <- Corpus(VectorSource(data_reviews))
# === Data Cleaning ===
# Membersihkan URL
deleteURL <- function(x) gsub("http[^[:space:]]*", "", x)
cleanReview <- tm_map(data_reviews2, deleteURL)
#membersihkan New Line
deleteNewLine <- function(y) gsub("\n", "", y)
cleanReview <- tm_map(cleanReview, deleteNewLine)
# Membersihkan tanda koma
replaceComma <- function(y) gsub(",", "", y)
cleanReview <- tm_map(cleanReview, replaceComma)
# Membersihkan tanda titik dua
deleteColon <- function(y) gsub(":", "", y)
cleanReview <- tm_map(cleanReview, deleteColon)
# Membersihkan tanda titik koma
deleteSemiColon <- function(y) gsub(";", " ", y)
cleanReview <- tm_map(cleanReview, deleteSemiColon)
# Membersihkan tanda titik tiga
deleteEllipsis <- function(y) gsub("p…", "", y)
cleanReview <- tm_map(cleanReview, deleteEllipsis)
# Membersihkan amp
deleteAmp <- function(y) gsub("&amp;", "", y)
cleanReview <- tm_map(cleanReview, deleteAmp)
# Membersihkan karakter
deleteUN <- function(z) gsub("@\\w+", "", z)
cleanReview <- tm_map(cleanReview, deleteUN)
remove.all <- function(xy) gsub("[^[:alpha:][:space:]]*", "", xy)
# Membersihkan Tanda Baca
cleanReview <- tm_map(cleanReview,remove.all)
cleanReview <- tm_map(cleanReview, removePunctuation)
cleanReview <- tm_map(cleanReview, tolower)
Stopwords = readLines("stopwords.txt")
cleanReview <- tm_map(cleanReview,removeWords,Stopwords)
dataframe<-data.frame(text=unlist(sapply(cleanReview, `[`)), stringsAsFactors=F)
View(dataframe)
# Menyimpan data yang telah dibersihkan
write.csv(dataframe,file = 'clean_reviews.csv')
# Library untuk corpus dalam cleaning data
library(tm)
library(RTextTools)
# Library algoritma naivebayes
library(e1071)
library(dplyr)
library(caret)
#membaca file csv
df <- read.csv("clean_reviews.csv",stringsAsFactors = FALSE)
#melihat tipe dan juga struktur objek
glimpse(df)
#Mengatur seed generator bilangan acak R(berfungsi agar bilangan acak R akan tetap sama walau diproduksi berulang kali)
set.seed(20)
df <- df[sample(nrow(df)),]
df <- df[sample(nrow(df)),]
glimpse(df)
corpus <- Corpus(VectorSource(df$text))
corpus
inspect(corpus[1:10])
# Membersihkan kembali data yang tidak dibutuhkan
corpus.clean <- corpus%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords,stopwords(kind="en"))%>%
tm_map(stripWhitespace)
dtm <- DocumentTermMatrix(corpus.clean)
inspect(dtm[1:10,1:20])
#Untuk klasifikasi data menggunakan data training dan data test
df.train <- df[1:50,]
df.test <- df[51:100,]
dtm.train <- dtm[1:50,]
dtm.test <- dtm[51:100,]
corpus.clean.train <- corpus.clean[1:50]
corpus.clean.test <- corpus.clean[51:100]
#Kemudian teks akan diubah menjadi DTM(Document-Term Matrix) dengan proses tokenization -> akan mengubah kalimat menjadi beberapa term(1 kata, 2 kata, dst).
dim(dtm.train)
fivefreq <- findFreqTerms(dtm.train,5)
length(fivefreq)
dtm.train.nb <- DocumentTermMatrix(corpus.clean.train,control = list(dictionary=fivefreq))
#dim(dtm.train.nb)
dtm.test.nb <- DocumentTermMatrix(corpus.clean.test,control = list(dictionary=fivefreq))
dim(dtm.test.nb)
convert_count <- function(x){
y <- ifelse(x>0,1,0)
y <- factor(y,levels=c(0,1),labels=c("no","yes"))
y
}
#Data train dan test naive bayes
trainNB <- apply(dtm.train.nb,2,convert_count)
testNB <- apply(dtm.test.nb,1,convert_count)
#menampilkan wordcloud
library(wordcloud)
wordcloud(corpus.clean, min.freq = 4, max.words=100, random.order=F, colors=brewer.pal(8,"Dark2"))
#library yang dibutuhkan
library(shiny)
library(here)
library(vroom)
library(dplyr)
library(ggplot2)
library(plotly)
library(syuzhet)
library(wordcloud)
#membaca data csv
Alexa_review<- vroom(here("clean_reviews.csv"))
Alexa_review <- data.frame(Alexa_review)
#bagian UI
ui <- fluidPage(
titlePanel("Analisis Sentimen Terhadap Amazon Alexa"),
mainPanel(
tabsetPanel(type = "tabs",
# Data Review
tabPanel("Data Amazon Alexa", DT::dataTableOutput('tbl2')),
# Data Clean
tabPanel("Data Clean", DT::dataTableOutput('tbl')),
# Barplot
tabPanel("Barplot", plotOutput("scatterplot")),
# Word Cloud
tabPanel("Wordcloud", plotOutput("Wordcloud"))
)
)
)
# SERVER
server <- function(input, output) {
# Output Data Tabel
output$tbl2 = DT::renderDataTable({
DT::datatable(dataset <-
vroom(here('Amazon_Alexa_Review.csv')), options =
list(lengthChange = FALSE))
})
# Output Data Clean
output$tbl = DT::renderDataTable({
DT::datatable(Alexa_review, options = list(lengthChange =
FALSE))
})
# Output Plot
output$scatterplot <- renderPlot({
par(mar=rep(3,4))
a <- barplot(colSums(s),col=rainbow(10),ylab='count',main='Sentimen
Analisis')}, height=400)
df <- read.csv("clean_reviews.csv",stringsAsFactors = FALSE)
glimpse(df)
# Set the seed of R‘s random number generator, which is useful for creating simulations or random objects that can be reproduced.
set.seed(20)
df <- df[sample(nrow(df)),]
df <- df[sample(nrow(df)),]
glimpse(df)
corpus <- Corpus(VectorSource(df$text))
corpus
inspect(corpus[1:10])
# Membersihkan data yang tidak dibutuhkan
corpus.clean <- corpus%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords,stopwords(kind="en"))%>%
tm_map(stripWhitespace)
dtm <- DocumentTermMatrix(corpus.clean)
inspect(dtm[1:10,1:20])
df.train <- df[1:50,]
df.test <- df[51:100,]
dtm.train <- dtm[1:50,]
dtm.test <- dtm[51:100,]
corpus.clean.train <- corpus.clean[1:50]
corpus.clean.test <- corpus.clean[51:100]
dim(dtm.train)
fivefreq <- findFreqTerms(dtm.train,5)
length(fivefreq)
dtm.train.nb <- DocumentTermMatrix(corpus.clean.train,control = list(dictionary=fivefreq))
#dim(dtm.train.nb)
dtm.test.nb <- DocumentTermMatrix(corpus.clean.test,control = list(dictionary=fivefreq))
dim(dtm.test.nb)
convert_count <- function(x){
y <- ifelse(x>0,1,0)
y <- factor(y,levels=c(0,1),labels=c("no","yes"))
y
}
trainNB <- apply(dtm.train.nb,2,convert_count)
testNB <- apply(dtm.test.nb,1,convert_count)
# Output WordCloud
output$Wordcloud <- renderPlot({
wordcloud(corpus.clean,min.freq =
4,max.words=100,random.order=F,colors=brewer.pal(8,"Dark2"))
})
}
# Program Shiny
shinyApp(ui = ui, server = server)
#library yang dibutuhkan
library(shiny)
library(here)
library(vroom)
library(dplyr)
library(ggplot2)
library(plotly)
library(syuzhet)
library(wordcloud)
#membaca data csv
Alexa_review<- vroom(here("clean_reviews.csv"))
Alexa_review <- data.frame(Alexa_review)
#bagian UI
ui <- fluidPage(
titlePanel("Analisis Sentimen Terhadap Amazon Alexa"),
mainPanel(
tabsetPanel(type = "tabs",
# Data Review
tabPanel("Data Amazon Alexa", DT::dataTableOutput('tbl2')),
# Data Clean
tabPanel("Data Clean", DT::dataTableOutput('tbl')),
# Barplot
tabPanel("Barplot", plotOutput("scatterplot")),
# Word Cloud
tabPanel("Wordcloud", plotOutput("Wordcloud"))
)
)
)
# SERVER
server <- function(input, output) {
# Output Data Tabel
output$tbl2 = DT::renderDataTable({
DT::datatable(dataset <-
vroom(here('Amazon_Alexa_Review.csv')), options =
list(lengthChange = FALSE))
})
# Output Data Clean
output$tbl = DT::renderDataTable({
DT::datatable(Alexa_review, options = list(lengthChange =
FALSE))
})
# Output Plot
output$scatterplot <- renderPlot({
par(mar=rep(3,4))
a <- barplot(colSums(emotion),col=rainbow(10),ylab='count',main='Sentimen
Analisis')}, height=400)
df <- read.csv("clean_reviews.csv",stringsAsFactors = FALSE)
glimpse(df)
# Set the seed of R‘s random number generator, which is useful for creating simulations or random objects that can be reproduced.
set.seed(20)
df <- df[sample(nrow(df)),]
df <- df[sample(nrow(df)),]
glimpse(df)
corpus <- Corpus(VectorSource(df$text))
corpus
inspect(corpus[1:10])
# Membersihkan data yang tidak dibutuhkan
corpus.clean <- corpus%>%
tm_map(content_transformer(tolower))%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords,stopwords(kind="en"))%>%
tm_map(stripWhitespace)
dtm <- DocumentTermMatrix(corpus.clean)
inspect(dtm[1:10,1:20])
df.train <- df[1:50,]
df.test <- df[51:100,]
dtm.train <- dtm[1:50,]
dtm.test <- dtm[51:100,]
corpus.clean.train <- corpus.clean[1:50]
corpus.clean.test <- corpus.clean[51:100]
dim(dtm.train)
fivefreq <- findFreqTerms(dtm.train,5)
length(fivefreq)
dtm.train.nb <- DocumentTermMatrix(corpus.clean.train,control = list(dictionary=fivefreq))
#dim(dtm.train.nb)
dtm.test.nb <- DocumentTermMatrix(corpus.clean.test,control = list(dictionary=fivefreq))
dim(dtm.test.nb)
convert_count <- function(x){
y <- ifelse(x>0,1,0)
y <- factor(y,levels=c(0,1),labels=c("no","yes"))
y
}
trainNB <- apply(dtm.train.nb,2,convert_count)
testNB <- apply(dtm.test.nb,1,convert_count)
# Output WordCloud
output$Wordcloud <- renderPlot({
wordcloud(corpus.clean,min.freq =
4,max.words=100,random.order=F,colors=brewer.pal(8,"Dark2"))
})
}
# Program Shiny
shinyApp(ui = ui, server = server)
